{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtencion del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf data\n",
    "#!git clone https://github.com/juanma1982/GANNets.git data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 23692\r\n",
      "-rw-r----- 1 gaston gaston 242 dic 10  2015 10005.png\r\n",
      "-rw-r----- 1 gaston gaston 280 dic 10  2015 1000.png\r\n",
      "-rw-r----- 1 gaston gaston 174 dic 10  2015 10010.png\r\n",
      "-rw-r----- 1 gaston gaston 287 dic 10  2015 10022.png\r\n",
      "-rw-r----- 1 gaston gaston 286 dic 10  2015 10025.png\r\n",
      "-rw-r----- 1 gaston gaston 313 dic 10  2015 10026.png\r\n",
      "-rw-r----- 1 gaston gaston 281 dic 10  2015 10045.png\r\n",
      "-rw-r----- 1 gaston gaston 280 dic 10  2015 10069.png\r\n",
      "-rw-r----- 1 gaston gaston 309 dic 10  2015 10071.png\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/mnist_png/training/0 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_DIR = os.path.join(\"data\", \"mnist_png\", \"training\")\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "# Number of inputs counting both mnist data and generated data for the discriminator, and number of random inputs for\n",
    "# the generator\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "GEN_LEARNING_RATE = 1e-4\n",
    "DISC_LEARNING_RATE = 1e-4\n",
    "\n",
    "LATENT_SPACE_SHAPE = 100\n",
    "\n",
    "GEN_VARIABLE_SCOPE = \"generator\"\n",
    "DISC_VARIABLE_SCOPE = \"discriminator\"\n",
    "\n",
    "MAX_STEPS = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones del modelo y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(latent_space, label, training=True):\n",
    "    \"\"\"\n",
    "    Defines the generator network using the latent_space as input.\n",
    "    Args:\n",
    "        latent_space: input for the generator network\n",
    "        label: 10 dimensioanl one-hot tensor\n",
    "    Returns:\n",
    "        Generated images\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(GEN_VARIABLE_SCOPE):\n",
    "        net = tf.concat([latent_space, label], axis=1)\n",
    "        net = tf.layers.dense(net, 7 * 7 * 64, activation=None, use_bias=False)\n",
    "        net = tf.layers.batch_normalization(net, training=training)\n",
    "\n",
    "        # 7 x 7\n",
    "        net = tf.reshape(net, [-1, 7, 7, 64])\n",
    "\n",
    "        # 7 x 7\n",
    "        net = tf.layers.conv2d_transpose(net, 64, kernel_size=(5, 5),\n",
    "                                         strides=(1, 1),\n",
    "                                         activation=None,\n",
    "                                         padding='same',\n",
    "                                         use_bias=False)\n",
    "        net = tf.layers.batch_normalization(net, training=training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        # 14 x 14\n",
    "        net = tf.layers.conv2d_transpose(net, 32, kernel_size=(5, 5),\n",
    "                                         strides=(2, 2),\n",
    "                                         activation=None,\n",
    "                                         padding='same',\n",
    "                                         use_bias=False)\n",
    "        net = tf.layers.batch_normalization(net, training=training)\n",
    "        net = tf.nn.leaky_relu(net)\n",
    "\n",
    "        # 28 x 28\n",
    "        images = tf.layers.conv2d_transpose(net, 1, kernel_size=(5, 5),\n",
    "                                            strides=(2, 2),\n",
    "                                            activation=tf.nn.sigmoid,\n",
    "                                            padding='same',\n",
    "                                            use_bias=False)\n",
    "        return images\n",
    "\n",
    "\n",
    "def discriminator(images, label, training=True):\n",
    "    \"\"\"Defines the discriminator network\n",
    "    Args:\n",
    "        images: input images as 28x28 tensors\n",
    "        label: 10 dimensioanl one-hot tensor\n",
    "    Returns:\n",
    "        Logits and prediction for each image\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(DISC_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
    "        net = images\n",
    "        net = tf.layers.conv2d(net, 64, kernel_size=(5, 5), strides=(2, 2),\n",
    "                               activation=tf.nn.leaky_relu, padding='same')\n",
    "        net = tf.layers.dropout(net, training=training)\n",
    "\n",
    "        net = tf.layers.conv2d(net, 128, kernel_size=(5, 5), strides=(2, 2),\n",
    "                               activation=tf.nn.leaky_relu, padding='same')\n",
    "        net = tf.layers.dropout(net, training=training)\n",
    "\n",
    "        net_shape = net.shape\n",
    "        net_reshaped = tf.reshape(net, [-1,\n",
    "                                        net_shape[1] * net_shape[2] * net_shape[\n",
    "                                            3]])\n",
    "        net_with_label = tf.concat([net_reshaped, label], axis=1)\n",
    "        logits = tf.layers.dense(net_with_label, 1,\n",
    "                             activation=None)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def _parse_function(filename, label):\n",
    "    \"\"\"\n",
    "    Reads an image from a file, decodes it into a dense tensor, and resizes it to a fixed shape.\n",
    "    \"\"\"\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_png(image_string)\n",
    "    image_resized = tf.reshape(image_decoded, [28, 28, 1])\n",
    "    return tf.cast(image_resized, tf.float32) / 255, label\n",
    "\n",
    "\n",
    "def _mnist_filenames_and_labels():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        A tuple of lists, where the first list contains the mnist png file paths, and the second list contains the\n",
    "        label for each image.\n",
    "    \"\"\"\n",
    "    images_paths = []\n",
    "    images_labels = []\n",
    "    for label in range(10):\n",
    "        images_dir = os.path.join(TRAINING_DATA_DIR, str(label))\n",
    "        current_images_paths = os.listdir(images_dir)\n",
    "        images_paths += list(\n",
    "            map(lambda image_path: os.path.join(images_dir, image_path),\n",
    "                current_images_paths))\n",
    "        images_labels += [label] * len(current_images_paths)\n",
    "    return images_paths, images_labels\n",
    "\n",
    "\n",
    "def shuffle(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "  \n",
    "def _generator_step(sess):\n",
    "    latent_space_np = np.random.randn(BATCH_SIZE, LATENT_SPACE_SHAPE)\n",
    "    label = np.random.randint(10, size=BATCH_SIZE)\n",
    "    _, G_loss_np, step_value = sess.run([G_optimizer, G_loss, step],\n",
    "                            feed_dict={latent_space: latent_space_np,\n",
    "                                       G_label: label})\n",
    "    if step_value % 97 == 0:\n",
    "        print()\n",
    "        print(\"Step: \", sess.run(step))\n",
    "        print(\"G_loss: \", G_loss_np)\n",
    "\n",
    "\n",
    "def _discriminator_step(sess):\n",
    "    latent_space_np = np.random.randn(BATCH_SIZE // 2, LATENT_SPACE_SHAPE)\n",
    "    label = np.random.randint(10, size=BATCH_SIZE // 2)\n",
    "    _, D_loss_np, step_value = sess.run([D_optimizer, D_loss, step],\n",
    "                            feed_dict={latent_space: latent_space_np,\n",
    "                                       G_label: label})\n",
    "    if step_value % 97 == 0:\n",
    "        print()\n",
    "        print(\"Step: \", sess.run(step))\n",
    "        print(\"D_loss: \", D_loss_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del grafo de TF y ejecución de la Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph is finalized and cannot be modified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d8b318120416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_or_create_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_parse_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   1185\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m       ])\n\u001b[0;32m-> 1187\u001b[0;31m       \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0mbatch_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m   6047\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6048\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6049\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6050\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# False values do not suppress exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't stop after throw()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4012\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4013\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4014\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnew_stack\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnew_stack\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4015\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4016\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m   1183\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[1;32m   1184\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[0;32m-> 1185\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m       ])\n\u001b[1;32m   1187\u001b[0m       \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1183\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[1;32m   1184\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[0;32m-> 1185\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m       ])\n\u001b[1;32m   1187\u001b[0m       \u001b[0mflat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1046\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    226\u001b[0m                                          as_ref=False):\n\u001b[1;32m    227\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    211\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[1;32m    212\u001b[0m              \"dtype\": dtype_value},\n\u001b[0;32m--> 213\u001b[0;31m       name=name).outputs[0]\n\u001b[0m\u001b[1;32m    214\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3242\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3244\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_finalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3245\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/virtualenvs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_check_not_finalized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2915\u001b[0m     \"\"\"\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2917\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Graph is finalized and cannot be modified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2919\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph is finalized and cannot be modified."
     ]
    }
   ],
   "source": [
    "filenames, labels = _mnist_filenames_and_labels()\n",
    "filenames, labels = shuffle(np.array(filenames), np.array(labels))\n",
    "\n",
    "step = tf.train.get_or_create_global_step()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "\n",
    "# The other half of the batch will come from the generator.\n",
    "dataset = dataset.batch(batch_size=BATCH_SIZE // 2)\n",
    "\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Iterator for tuples of images and labels\n",
    "next = iterator.get_next()\n",
    "\n",
    "latent_space = tf.placeholder(tf.float32, shape=[None, LATENT_SPACE_SHAPE])\n",
    "G_label = tf.placeholder(tf.int32, shape=[None])\n",
    "G_label_one_hot = tf.one_hot(G_label, 10)\n",
    "G_images = generator(latent_space, G_label_one_hot)\n",
    "\n",
    "D_fake_logits = discriminator(G_images, G_label_one_hot)\n",
    "\n",
    "real_image = next[0]\n",
    "real_label = tf.one_hot(next[1], 10)\n",
    "D_real_logits = discriminator(real_image, real_label)\n",
    "\n",
    "G_expected = tf.ones_like(D_fake_logits)\n",
    "G_loss = tf.losses.sigmoid_cross_entropy(G_expected, D_fake_logits)\n",
    "\n",
    "D_real_expected = tf.ones_like(D_real_logits)\n",
    "D_fake_expected = tf.zeros_like(D_fake_logits)\n",
    "\n",
    "D_real_loss = tf.losses.sigmoid_cross_entropy(D_real_expected, D_real_logits)\n",
    "D_fake_loss = tf.losses.sigmoid_cross_entropy(D_fake_expected, D_fake_logits)\n",
    "D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "G_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                GEN_VARIABLE_SCOPE)\n",
    "\n",
    "G_optimizer = tf.train.AdamOptimizer(\n",
    "    learning_rate=GEN_LEARNING_RATE).minimize(G_loss, var_list=G_variables,\n",
    "                                              global_step=tf.train.get_global_step())\n",
    "\n",
    "D_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                DISC_VARIABLE_SCOPE)\n",
    "\n",
    "D_optimizer = tf.train.AdamOptimizer(\n",
    "    learning_rate=DISC_LEARNING_RATE).minimize(D_loss, var_list=D_variables,\n",
    "                                               global_step=tf.train.get_global_step())\n",
    "\n",
    "tf.summary.scalar(\"Gen loss\", G_loss, family=\"Generator\")\n",
    "tf.summary.scalar(\"Disc loss\", D_loss, family=\"Discriminator\")\n",
    "tf.summary.image(\"Gen images\", G_images, max_outputs=1)\n",
    "\n",
    "\n",
    "hooks = [tf.train.StopAtStepHook(num_steps=MAX_STEPS)]\n",
    "\n",
    "with tf.train.MonitoredTrainingSession(checkpoint_dir=CHECKPOINT_DIR,\n",
    "                                       hooks=hooks) as sess:\n",
    "    while not sess.should_stop():\n",
    "        _generator_step(sess)\n",
    "        _discriminator_step(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
